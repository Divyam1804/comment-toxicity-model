# -*- coding: utf-8 -*-
"""comment_toxicity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10bFu9Wo4-5V6Owm9w_8t8PazBXzrEkRM

# 1. Problem Statement

We are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:

* toxic
* severe_toxic
* obscene
* threat
* insult
* identity_hate

We must create a model which predicts a probability of each type of toxicity for each comment.

It is a Supervised Learning and Multiclass Classification Problem.
"""

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import pandas as pd
import numpy as np
import tensorflow as tf
import os
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""#2. Data Collection

For our dataset we are using Kaggle dataset provided by Jigsaw under their competition.

Dataset Link-https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data
"""

df=pd.read_csv("/content/drive/MyDrive/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv")

df.head()

#Example of a toxic comment in our dataset
df.iloc[12]

"""#3. Data Preprocessing

Here it involves following steps:



*   Tokenization-https://vaclavkosar.com/ml/Tokenization-in-Machine-Learning-Explained
*   Creation of Training,Test and Validation set


"""

#Tokenization
from tensorflow.keras.layers import TextVectorization

x=df["comment_text"]
y=df[df.columns[2:]].values #conerts int a numpy array.

x.head()

y

MAX_FEATURES=200000 #No. of words in Vocab of machine's language.

vectorizer = TextVectorization(max_tokens=MAX_FEATURES,
                            standardize="lower_and_strip_punctuation",
                             output_sequence_length=1800,
                             output_mode='int')

vectorizer.adapt(x.values)

vectorizer.get_vocabulary()

#Example of working vectorizer encoding a scentence
vectorizer("Hello World, life is Great")[:5]

vectorized_text = vectorizer(x.values)

vectorized_text # the shape tells us that we have size of 159751 scentences, each scentence with max length of 1800 words and if some scentences are shorter, we fill it out with 0s

# Creating a dataset generation pipeline
dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,y))
dataset=dataset.cache()
dataset=dataset.shuffle(160000)
dataset=dataset.batch(256)
dataset=dataset.prefetch(2)

dataset.as_numpy_iterator().next()

len(dataset)

#Creation of Training,Test and Validation set
train = dataset.take(int(len(dataset)*.7))
val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))
test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))

train_generator=train.as_numpy_iterator()

train_generator.next()

"""#4. Model Creation



*   Embedding - repersentation of word with certain features as integer or floats.
*   We create a Sequential model, use LSTM layer.

For more info, refer to - https://www.tensorflow.org/guide/keras/working_with_rnns#:~:text=There%20are%20three%20built%2Din,be%20fed%20to%20next%20timestep.


"""

from tensorflow.keras.layers import LSTM, Dropout, Bidirectional,Dense,Embedding
from tensorflow.keras.models import Sequential

# We are instaniating a neural network and add hidden layers in neural network,accordingily.
model = Sequential()
#Creating Embedding Layer
model.add(Embedding(MAX_FEATURES+1,32))
# Bidirectional LSTM Layer
model.add(Bidirectional(LSTM(32,activation='tanh')))
# Feature Extractor Fully connected layers
model.add(Dense(128,activation='relu'))
model.add(Dense(256,activation='relu'))
model.add(Dense(128,activation='relu'))
#Output Mapping layers
model.add(Dense(6,activation='sigmoid'))

model.compile(loss='BinaryCrossentropy',optimizer='Adam')

model.summary()

history=model.fit(train,epochs=20,validation_data=val)

history.history

plt.figure(figsize=(8,5))
pd.DataFrame(history.history).plot()
plt.show()

"""#5. Making Predictions

To make predictions on our model, we need to tokenize our test input.
"""

df.columns[2:]

input_text=vectorizer("You are terrible")
input_text[:3]

batch=test.as_numpy_iterator().next()

np.expand_dims(input_text,0) #We do so because our model is expecting a batch as input rather than a single text

res=model.predict(np.expand_dims(input_text,0))
res

# batch prediction
batch_x,batch_y=test.as_numpy_iterator().next()

(model.predict(batch_x)>0.5).astype(int)

"""#6. Evaluating Model

Since, we have multiple binary outputs,we will use binary metrics like **precision** and **recall**.
"""

from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy

pre=Precision()
re=Recall()
acc=CategoricalAccuracy()

#setting up test pipeline
for batch in test.as_numpy_iterator():
  #Unpack the batch
  x_true, y_true = batch

  #Make a prediction
  y_predict=model.predict(x_true)

  #Flatten the predictions
  y_true=y_true.flatten()
  y_predict=y_predict.flatten()

  #Updating the scores
  pre.update_state(y_true,y_predict)
  re.update_state(y_true,y_predict)
  acc.update_state(y_true,y_predict)

print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')

"""#7. Model Deployment

In this, we use Gradio library from Python to create a web application for our model.
"""

!pip install gradio jinja2

import gradio as gr

def score_comment(comment):
  vectorized_comment=vectorizer([comment])
  results=model.predict(vectorized_comment)

  text=''
  for idx,col in enumerate(df.columns[2:]):
    text+='{} : {}\n'.format(col,results[0][idx]>0.5)

  return text

interface=gr.Interface(fn=score_comment,
                       inputs=gr.inputs.Textbox(lines=2,placeholder="Enter the comment"),
                       outputs='text')

interface.launch(share=True)